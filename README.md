# AMRL-Meta-Net
Hyperparameter configuration remains a critical yet challenging issue in reinforcement learning, as effective settings depend on evolving training dynamics and task-specific characteristics. Most existing approaches rely on static, offline optimization or population-based strategies, which either fail to adapt during training or require substantial computational resources. From an expert systems perspective, hyperparameter control can be viewed as a sequential decision-making problem under uncertainty, where decisions should be continuously revised based on observed system feedback.
Adaptive Meta-Reinforcement Learning (AMRL), a resource-efficient adaptive expert decision framework for online hyperparameter control. AMRL adopts a dual-loop architecture in which a meta-level expert module, implemented as an LSTM-based meta-learner. By explicitly modeling hyperparameter adaptation as a sequential decision process, the proposed framework enables continuous, experience-driven adjustment using a single learning agent, without relying on population-based training or extensive parallel computation.

AMRL ‚Äî Adaptive Meta-Reinforcement Learning

This repository contains the official implementation of the paper:

Adaptive Meta-Reinforcement Learning for Online Hyperparameter Optimization

The codebase provides a fully reproducible, Google-Colab-ready implementation of the proposed AMRL framework, along with standard reinforcement learning baselines and benchmark environments.


üîç Overview

The core idea of AMRL is to adapt reinforcement learning hyperparameters online, during training, using a learned meta-controller.
Unlike static or grid-based tuning, AMRL continuously updates learning rate, discount factor, and entropy regularization based on observed learning dynamics.

Key features:
	‚Ä¢	Online meta-learning of hyperparameters
	‚Ä¢	Compatible with PPO, A2C, and DQN
	‚Ä¢	Supports Gym and bsuite benchmarks
	‚Ä¢	One-click Google Colab reproduction
	‚Ä¢	Deterministic, logged, and reviewer-friendly


üß† Algorithms Implemented

Meta-Learning
	‚Ä¢	AMRL (proposed)
	‚Ä¢	LSTM-based meta-learner
	‚Ä¢	PPO-style meta-optimization

Baselines
	‚Ä¢	PPO
	‚Ä¢	A2C
	‚Ä¢	DQN


üî¨ Hyperparameter Optimization with Optuna

Following the paper, Optuna is used for offline initialization of base hyperparameters prior to meta-learning.
	‚Ä¢	Optuna performs a coarse search over (Œ±, Œ≥, Œ≤)
	‚Ä¢	AMRL then adapts these parameters online
	‚Ä¢	Optuna is not used inside the training loop

The Optuna implementation is located at:
